{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.简单层的实现\n",
    "\n",
    "我们把要实现的计算图的乘法节点称为\"乘法层\"(MulLayer)，加法节点称为\"加法层\"（AddLayer）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#乘法层的实现\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        self.x =x\n",
    "        self.y =y\n",
    "        out = x*y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout * self.y # 翻转x和y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx,dy\n",
    "    \n",
    "#__init__()中会初始化实例变量x和y,它们用于保存正向传播时的输入值。forward()接受x和y两个参数，将它们相乘后输出。\n",
    "#backward()将从上游传来的导数（dout）乘以正向传播的翻转值，然后传给下游。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<img src=\"img/buy2apple.png\" width=\"80%\">\n",
    "购买2个苹果的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "#用MulLayer实现上图中的购买2个苹果计算图的前向传播和反向传播\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax =1.1\n",
    "\n",
    "#layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price = mul_apple_layer.forward(apple,apple_num)\n",
    "price = mul_tax_layer.forward(apple_price,tax)\n",
    "\n",
    "print(price) #220\n",
    "\n",
    "#backward\n",
    "dprice =1\n",
    "dapple_price,dtax = mul_tax_layer.backward(dprice)\n",
    "dapple,dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple,dapple_num,dtax) #2.2 110 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加法层的实现\n",
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "#加法层不需要特意进行初始化，所以_init__()中什么也不运行。加法层的forward()接收x和y两个参数，将它们相加后输出。\n",
    "#backward()将上游传来的导数（dout）原封不动地传递给下游。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/buy2appleand3orange.png\" width =\"80%\">\n",
    "购买2个苹果和3个橘子的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "#用MulLayer和AddLayer实现购买两个苹果和3个橘子计算图的前向传播和反向传播\n",
    "\n",
    "apple = 100\n",
    "apple_num =2\n",
    "orange =150\n",
    "orange_num =3\n",
    "tax = 1.1\n",
    "\n",
    "#layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#forward\n",
    "apple_price = mul_apple_layer.forward(apple,apple_num) #(1)\n",
    "orange_price = mul_orange_layer.forward(orange,orange_num) #(2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price,orange_price) #(3)\n",
    "price = mul_tax_layer.forward(all_price,tax) #(4)\n",
    "\n",
    "#backward\n",
    "dprice = 1\n",
    "dall_price,dtax = mul_tax_layer.backward(dprice) #(4)\n",
    "dapple_price,dorange_price = add_apple_orange_layer.backward(dall_price) #(3)\n",
    "dorange,dorange_num = mul_orange_layer.backward(dorange_price) #(2)\n",
    "dapple,dapple_num = mul_apple_layer.backward(dapple_price) #(1)\n",
    "\n",
    "print(price) #715\n",
    "print(dapple_num,dapple,dorange,dorange_num,dtax) #110 2.2 3.3 165 650\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.激活函数的实现\n",
    "### ReLU层\n",
    "<img src=\"img/ReLU.png\" width=\"80%\">\n",
    "如果正向传播时的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将停在此处。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU层的计算图\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask =None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "#Relu类有实例变量mask。这个变量mask是由True/False构成的NumPy数组，他会把正向传播时的输入x的元素中小于等于0的地方保存为True,其他地方（大于0的元素）保存为False。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid层\n",
    "<img src=\"img/Sigmoid.png\" width=\"80%\">\n",
    "Sigmoid 层的计算图：可以根据正向传播的输出 y 计算反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid层的python实现\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self,x):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = 1/(1+np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backword(self,dout):\n",
    "        dx = dout*(1.0 - self.out) * self.out\n",
    "        \n",
    "        return dx\n",
    "\n",
    "#这个实现中，正向传播时将输出保存在了实例变量out中。然后，反向传播时，使用该变量out进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Affine/Softmax层的实现\n",
    "\n",
    "### Affine层\n",
    "神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为\"仿射变换\"。几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。因此，这里将进行仿射变换的处理实现为\"Affine层\"。\n",
    "<img src=\"img/Affine.png\" width=\"80%\">\n",
    "Affine层的计算图 变量是矩阵，各个变量上方标记了该变量的形状。反向传播时要注意变量的形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批版本的Affine层\n",
    "<img src=\"img/batchAffine.png\" width=\"80%\">\n",
    "批版本的Affine层的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#考虑输入数据为张量的Affine pythonS实现\n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 权重和偏置参数的导数\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward (self,x):\n",
    "        #对应张量\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        self.x = x\n",
    "        out = np.dot(x,self.W)+self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0)\n",
    "        \n",
    "        dx =dx.reshape(*self.original_x_shape) #还原输入数据的形状（对应张量）\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax-with-Loss层\n",
    "在Softmax-with-Loss层的计算图中，softmax函数记为Softmax层，交叉熵误差记为Cross Entropy Error层。\n",
    "<img src=\"img/SoftmaxwithLoss.png\" width=\"80%\">\n",
    "“简易版”的 Softmax-with-Loss 层的计算图\n",
    "\n",
    "神经网络的反向传播会把Softmax层的输出和监督标签的差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None #损失\n",
    "        self.y = None #softmax的输出\n",
    "        self.t = None #监督数据（one-hot vector）\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y  = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size =self.t.shape[0]\n",
    "        dx = (self.y -self.t)/batch_size\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 溢出对策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7 误差反向传播法的实现\n",
    "误差反向传播法可以快速高效地计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用误差反向传播法的python实现\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x:输入数据, t:监督数据\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.5119621105645954e-10\n",
      "b1:2.75015110431996e-09\n",
      "W2:5.974973264157114e-09\n",
      "b2:1.4045134996976128e-07\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "#反向传播的梯度确认\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1035 0.1029\n",
      "0.9036666666666666 0.9089\n",
      "0.9207666666666666 0.9234\n",
      "0.9355666666666667 0.9363\n",
      "0.9442833333333334 0.943\n",
      "0.9484 0.9473\n",
      "0.9549 0.9528\n",
      "0.9611166666666666 0.9568\n",
      "0.9640833333333333 0.9589\n",
      "0.9659166666666666 0.9616\n",
      "0.9687 0.9626\n",
      "0.9707833333333333 0.9652\n",
      "0.9735666666666667 0.9677\n",
      "0.9753833333333334 0.9676\n",
      "0.9750666666666666 0.9665\n",
      "0.9777 0.9694\n",
      "0.9779666666666667 0.9697\n"
     ]
    }
   ],
   "source": [
    "#使用误差反向传播法的学习\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 梯度\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
